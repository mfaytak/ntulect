<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"

 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

  <meta http-equiv="Content-Style-Type" content="text/css" />

  <meta name="generator" content="pandoc" />

  <meta name="author" content="Matthew Faytak University at Buffalo" />

  <title>NTU lectures (3)</title>

  <style type="text/css">

    code{white-space: pre-wrap;}

    span.smallcaps{font-variant: small-caps;}

    span.underline{text-decoration: underline;}

    div.column{display: inline-block; vertical-align: top; width: 50%;}

    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

    ul.task-list{list-style: none;}

    .display.math{display: block; text-align: center; margin: 0.5rem auto;}

  </style>

  <link rel="stylesheet" type="text/css" media="screen, projection, print"

    href="assets/styles/slidy.css" />

  <script src="assets/scripts/slidy.js"

    charset="utf-8" type="text/javascript"></script>

</head>

<body>

<div class="slide titlepage">

  <h1 class="title">NTU lectures (3)</h1>

  <p class="author">

Matthew Faytak<br/>University at Buffalo

  </p>

  <p class="date"><img src="./assets/media/UB_Stacked_Small.png" width="200"> <img src="./assets/media/ntu-logo.png" width="200"><br/><img src="./assets/media/qr1.png" width="170"></p>

</div>

<div id="ultrasound-basics" class="title-slide slide section level1">

<h1>ultrasound basics</h1>



</div>

<div id="the-basic-method" class="slide section level2">

<h1>the basic method</h1>

<p>How the method works: low intensity and very high frequency sound is transmitted through soft tissue and reflects back when the density of the medium changes</p>

<p>picture illustrating uti</p>

</div>

<div id="reflection" class="slide section level2">

<h1>reflection</h1>

<p>The signal in ultrasound data is the extent to which the high frequency energy reflects back to the probe</p>

<p>Occurs when the medium changes in density</p>

<ul>

<li>tissue tissue boundaries</li>

<li>tissue air boundaries</li>

</ul>

</div>

<div id="shadows" class="slide section level2">

<h1>shadows</h1>

<p>Rather than reflecting back energy, bone absorbs energy and casts a “shadow” in the ultrasound image</p>

<ul>

<li>hyoid shadow</li>

<li>mandible or chin shadow</li>

</ul>

<p>The two shadows delimit the useful range of data received from reflection</p>

<ul>

<li>this means the tongue tip is usually not visible in ultrasound data</li>

</ul>

</div>

<div id="rigid-landmarks" class="slide section level2">

<h1>Rigid landmarks</h1>

<p>A couple of rigid landmarks are often collected prior to recording the motion of the tongue</p>

<ul>

<li>palate trace, by doing a <em>swallow task</em></li>

<li>swallow water and watch as tongue suctions to roof of mouth</li>

<li>provides position of the hard palate, useful for gauging constriction degree</li>

<li>occlusal plane or bite plane, by having the speaker bite down on a rigid plate and press the tongue into it</li>

<li>provides common angle for representation of data from multiple speakers</li>

</ul>

</div>

<div id="stabilization" class="slide section level2">

<h1>Stabilization</h1>

<p>.</p>

</div>

<div id="a-single-frame" class="slide section level2">

<h1>a single frame</h1>

<p>The landmarks which we have discussed are visible in this sample frame captured in midsagittal section</p>

</div>

<div id="other-imaging-planes" class="slide section level2">

<h1>other imaging planes</h1>

<p>While we won’t discuss them here, it’s good to know that ultrasound is not constrained to mid sagittal images</p>

<ul>

<li>coronal slice can be taken by rotating the probe under the chin 90°</li>

<li>laryngeal ultrasound at an oblique angle is also possible</li>

</ul>

</div>

<div id="uses-of-ultrasound" class="slide section level2">

<h1>uses of ultrasound</h1>

<p>Generally used for imaging posture or shape of the tongue from root to blade, or its change over time</p>

<ul>

<li><em>broad</em> place and tongue position distinctions</li>

<li>complex tongue shapes (laterals, rhotics)</li>

<li>rapid tongue body and blade movements (flaps, clicks)</li>

</ul>

</div>

<div id="usage-in-the-field" class="slide section level2">

<h1>Usage in the field</h1>

<p>A critical tool for imaging vocal tract configurations in the field, outside of the lab</p>

<p>Non-invasive, generally not intimidating, and by far the most portable vocal track imaging technology</p>

<ul>

<li>no wall outlet required, draws power from laptop</li>

<li>relatively hands off, especially if stabilization headset is used</li>

</ul>

</div>

<div id="drawbacks" class="slide section level2">

<h1>drawbacks</h1>

<p>Only images the tongue in real time, no simultaneous imaging of hard palate possible, no imaging of larynx or velum</p>

<p>Image can be noisy, and not every potential participant images well</p>

<ul>

<li>small, young people image the best</li>

<li>beards can get in the way</li>

<li>probe can touch larger larynges, causing pain</li>

</ul>

</div>

<div id="data-type" class="slide section level2">

<h1>Data type</h1>

<p>Perhaps the biggest drawback is the data itself, which gives the raw reflectivity data; requires extensive post processing</p>

<p><em>Feature extraction</em>: extracting contours that represent the reflection at the tongue surface, or extracting motion of that contour along registration lines</p>

<ul>

<li>complex image processing, and manual intervention</li>

</ul>

<p>Next lecture: several ways around feature extraction</p>

</div>



<div id="ultrasound-feature-extraction" class="title-slide slide section level1">

<h1>ultrasound feature extraction</h1>



</div>

<div id="contour-extraction" class="slide section level2">

<h1>contour extraction</h1>

<p>By far the most common means of feature extraction for ultrasound data</p>

<ul>

<li>first principle: we know that reflection represents the surface of the tongue, which tells us something about the vocal tract area function</li>

</ul>

<p>Certainly not unreasonable</p>

</div>

<div id="a-brief-history" class="slide section level2">

<h1>a brief history</h1>

<p>The need to accurately extract contours is a long-standing problem for both general diagnostic and biomedical science and speech science so far as it concerns ultrasound imaging of the tongue</p>

<p>…</p>

</div>

<div id="manual-tracing" class="slide section level2">

<h1>manual tracing</h1>

<p>Most solid, but completely impractical</p>

</div>

<div id="semi-automated-tracing" class="slide section level2">

<h1>semi automated tracing</h1>

<p>Basically the mainstream</p>

<p>Still very time consuming, since extensive manual intervention is still required, especially if reflection signal is of less than ideal strength</p>

</div>

<div id="state-of-the-art" class="slide section level2">

<h1>State of the art</h1>

<p>Active snake models provide good performance and relatively easy computation</p>

<ul>

<li>examples, especially edge track</li>

</ul>

</div>

<div id="purely-automatic-tracing" class="slide section level2">

<h1>purely automatic tracing</h1>

<p>In recent years it has started to become possible to do completely automatic contour extraction</p>

<ul>

<li>SLURP</li>

<li>get contours?</li>

</ul>

<p>Per some recent work, these work surprisingly well, often as well as hand checked work</p>

</div>

<div id="registration-lines" class="slide section level2">

<h1>registration lines</h1>

<p>Lines drawn in a fan shaped grid out from probe origin</p>

<p>Example picture</p>

</div>

<div id="m--mode-ultrasound" class="slide section level2">

<h1>M- Mode ultrasound</h1>

<p>Registration lines mimic so-called m-mode ultrasound, commonly used to track rhythmic motion in biomedical contexts (i.e. heart or blood vessel motion)</p>

<p>image to demonstrate</p>

</div>

<div id="registration-line-uses" class="slide section level2">

<h1>registration line uses</h1>

<p>These can be used to track entire contours where they intersect the lines, but more commonly are used to track tongue motion in a small region of interest</p>

<p>Very common in tongue root retraction studies, or blade raising studies</p>

</div>



<div id="wrapping-up" class="title-slide slide section level1">

<h1>wrapping up</h1>



</div>

<div id="ultrasound-pros" class="slide section level2">

<h1>Ultrasound pros</h1>

<p>Most portable, least expensive, least invasive, often most familiar to participants</p>

<p>For fieldwork, it is really the only option</p>

</div>

<div id="ultrasound-cons" class="slide section level2">

<h1>ultrasound cons</h1>

<p>Messy, high dimensional data, which does not lend itself immediately to analysis</p>

<ul>

<li>feature extraction required, in the most typical approach, then those features are analyzed in place of the whole image</li>

<li>often very time consuming and requires a small army of annotators who must all agree on how to segment an image</li>

</ul>

</div>

<div id="next-lecture-some-solutions" class="slide section level2">

<h1>next lecture: some solutions</h1>

<p>Various ways around the feature extraction problem</p>

<p>Going in the direction of <strong>feature engineering</strong></p>

</div>

</body>

</html>
