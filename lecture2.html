<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"

 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

  <meta http-equiv="Content-Style-Type" content="text/css" />

  <meta name="generator" content="pandoc" />

  <meta name="author" content="Matthew Faytak University at Buffalo" />

  <title>NTU lectures (2)</title>

  <style type="text/css">

    code{white-space: pre-wrap;}

    span.smallcaps{font-variant: small-caps;}

    span.underline{text-decoration: underline;}

    div.column{display: inline-block; vertical-align: top; width: 50%;}

    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

    ul.task-list{list-style: none;}

    .display.math{display: block; text-align: center; margin: 0.5rem auto;}

  </style>

  <link rel="stylesheet" type="text/css" media="screen, projection, print"

    href="assets/styles/slidy.css" />

  <script src="assets/scripts/slidy.js"

    charset="utf-8" type="text/javascript"></script>

</head>

<body>

<div class="slide titlepage">

  <h1 class="title">NTU lectures (2)</h1>

  <p class="author">

Matthew Faytak<br/>University at Buffalo

  </p>

  <p class="date"><img src="./assets/media/UB_Stacked_Small.png" width="200"> <img src="./assets/media/ntu-logo.png" width="200"><br/><img src="./assets/media/qr1.png" width="170"></p>

</div>

<div id="overview" class="slide section level2">

<h1>Overview</h1>

<p>Last time, we discussed <strong>feature selection</strong></p>

<p>This time, we will discuss <strong>feature extraction</strong></p>

<ul>

<li>Especially using dimensionality reduction</li>

<li>Approach sometimes called “feature engineering”</li>

</ul>

</div>

<div id="feature-extraction" class="slide section level2">

<h1>Feature extraction</h1>

<p>Creation of <em>new features</em> from combination or rescaling/reweighting of other features</p>

<ul>

<li>As opposed to <em>selecting</em> a subset <span class="cite">Liu (2011)</span></li>

<li>Usually fewer in number than original features</li>

<li>Reduces the number of features required to describe a data set</li>

</ul>

</div>

<div id="outlook" class="slide section level2">

<h1>Outlook</h1>

<p>Different motivations sometimes underpin feature extraction</p>

<ul>

<li>Concerned with <em>performance</em> of feature as an input to some task</li>

<li>Especially frequent in <strong>machine learning</strong> and “artificial intelligence” applications</li>

</ul>

</div>

<div id="dimensionality-reduction" class="title-slide slide section level1">

<h1>Dimensionality reduction</h1>



</div>

<div id="dimensionality-reduction-1" class="slide section level2">

<h1>Dimensionality reduction</h1>

<p>Converts a high dimensional space consisting of numerous features into a lower dimensional space consisting of many fewer features</p>

<p>Technically, feature selection is also a way of reducing dimensionality <span class="cite">Liu (2011)</span>, though not often used in that sense (we are mainly discussing algorithmic approaches here)</p>

</div>

<div id="example" class="slide section level2">

<h1>example</h1>

<p>a simple metaphor/image: ball on spring from Shlens?</p>

</div>

<div id="curse-of-dimensionality" class="slide section level2">

<h1>Curse of dimensionality</h1>

<p>Adding a lot of dimensions exponentially increases the amount of space that needs to be covered, with numerous undesirable side effects <span class="cite">Bellman (1957); Keough &amp; Mueen (2011)</span></p>

<ul>

<li>Computation becomes exponentially more difficult</li>

<li>Exponentially more samples needed to evenly cover the space and maintain good performance</li>

</ul>

<p>Distance in very high-dimensional spaces space becomes nonsensical <span class="cite">Aggarwal et al (2001)</span></p>

<ul>

<li>Objects more or less all equidistant from one another</li>

</ul>

</div>

<div id="principal-component-analysis" class="slide section level2">

<h1>principal component analysis</h1>

<p>Eigenvectors: the latent dimensions discovered by PCA, which form a better basis for describing the data Eigenvalues: the amount of variance in data set explained by each eigenvector Projections (“scores”): transform of data into this new space, in terms of eigenvectors</p>

</div>

<div id="principal-component-analysis-1" class="slide section level2">

<h1>principal component analysis</h1>

<p>A basic example in NB</p>

</div>



<div id="black-box-methods" class="title-slide slide section level1">

<h1>“Black box” methods</h1>



</div>

<div id="why-this-lecture-doesnt-cover-neural-nets" class="slide section level2">

<h1>Why this lecture doesn’t cover neural nets</h1>

<p>If we are theoretically motivated, we must be able to <em>understand</em> human behavioral reasons for a certain outcome</p>

<p>NNs make this quite hard because they merely <em>perform</em></p>

</div>

<div id="transparent-dimensionality-reduction" class="slide section level2">

<h1>Transparent dimensionality reduction</h1>

<p>Styler paper notes</p>

</div>



<div id="wrapping-up" class="title-slide slide section level1">

<h1>Wrapping up</h1>



</div>

<div id="feature-extraction-advantages" class="slide section level2">

<h1>Feature extraction: advantages</h1>

<p>Speed, consistency/replicability, holistic description</p>

<p>Dimred can speed along discovery of new features to select</p>

</div>

<div id="feature-extraction-disadvantages" class="slide section level2">

<h1>Feature extraction: disadvantages</h1>

<p>New n-dimensional space may not be <em>interpretable</em>: what does a dimension “mean”?</p>

<ul>

<li>Dimensions may not correspond to anything linguistically relevant</li>

</ul>

</div>

<div id="next-time" class="slide section level2">

<h1>Next time</h1>

</div>

<div id="references" class="slide section level2 bib">

<h1 class="bib">References</h1>

<p>Aggarwal, C., Hinneburg, A. &amp; Keim, D. (2001). On the surprising behavior of distance metrics in high dimensional space. In <i>International Conference on Database Theory</i>, 420-434. Springer.</p>

<p>Bellman, R. (1957). <i>Dynamic Programming</i>. Princeton University Press.</p>

<p>Keogh, E. &amp; Mueen, A. (2011). Curse of dimensionality. In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 257-258. Springer.</p>

<p>Shlens</p>

<p>Styler</p>

</div>

</body>

</html>
