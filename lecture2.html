<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"

 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

  <meta http-equiv="Content-Style-Type" content="text/css" />

  <meta name="generator" content="pandoc" />

  <meta name="author" content="Matthew Faytak University at Buffalo" />

  <title>NTU lectures (2)</title>

  <style type="text/css">

      code{white-space: pre-wrap;}

      span.smallcaps{font-variant: small-caps;}

      span.underline{text-decoration: underline;}

      div.column{display: inline-block; vertical-align: top; width: 50%;}

  </style>

  <link rel="stylesheet" type="text/css" media="screen, projection, print"

    href="assets/styles/slidy.css" />

  <script src="assets/scripts/slidy.js"

    charset="utf-8" type="text/javascript"></script>

</head>

<body>

<div class="slide titlepage">

  <h1 class="title">NTU lectures (2)</h1>

  <p class="author">

Matthew Faytak<br/>University at Buffalo

  </p>

  <p class="date"><img src="./assets/media/UB_Stacked_Small.png" width="200"> <img src="./assets/media/ntu-logo.png" width="200"><br/><img src="./assets/media/qr1.png" width="170"></p>

</div>

<div id="overview" class="slide section level2">

<h1>Overview</h1>

<p>In the previous lecture, we discussed <strong>feature selection</strong></p>

<ul>

<li>Manually <em>selecting</em> a subset of features <span class="cite">see Liu (2011)</span></li>

</ul>

<p>Here, we will discuss <strong>feature extraction</strong></p>

<ul>

<li>Especially using dimensionality reduction <span class="cite">see Vlachos (2011)</span></li>

<li>Taking a large number of features and using them to generate a smaller number of features</li>

<li>Reduces the number of features required to describe a data set</li>

</ul>

</div>

<div id="outlook" class="slide section level2">

<h1>Outlook</h1>

<p>Some different motivations underpin feature extraction: <strong>data-driven</strong> rather than theory-driven</p>

<ul>

<li>Allows new features well-suited to task to emerge from data</li>

<li>Mainly concerned with <em>performance</em> of feature as an input to some task</li>

<li>Especially frequent in <strong>machine learning</strong> and ‚Äúartificial intelligence‚Äù applications</li>

</ul>

</div>

<div id="dimensionality-reduction" class="title-slide slide section level1"><h1>Dimensionality reduction</h1></div><div id="features-as-dimensions" class="slide section level2">

<h1>Features as dimensions</h1>

<p>Features (observable attributes) define <strong>dimensions</strong> in a data set</p>

<ul>

<li>i.e.¬†lower formants define a 3D space (F1 dimension, F2 dimension, F3 dimension)</li>

<li></li>

</ul>

<p>[3D vowel space plot]</p>

</div><div id="curse-of-dimensionality" class="slide section level2">

<h1>Curse of dimensionality</h1>

<p>If we have 1,000 features (dimensions) per observation, why not just use all of them? The <strong>curse of dimensionality</strong> üòà</p>

<p>Adding <em>many</em> dimensions exponentially increases the amount of space that needs to be covered, with numerous undesirable side effects <span class="cite">Bellman (1957); Keough &amp; Mueen (2011)</span></p>

<ul>

<li>Computation becomes exponentially more difficult</li>

<li>Exponentially more samples needed to evenly cover the space and maintain good performance</li>

</ul>

<p>Distance in very high-dimensional spaces becomes nonsensical <span class="cite">Aggarwal et al (2001)</span></p>

<ul>

<li>Objects become more or less equidistant from one another</li>

</ul>

</div><div id="dimensionality-reduction-1" class="slide section level2">

<h1>Dimensionality reduction</h1>

<p>Converts a high dimensional space consisting of numerous features into a lower dimensional space consisting of fewer features</p>

<ul>

<li>Technically, feature selection is also a way of reducing dimensionality <span class="cite">Liu (2011)</span></li>

<li>We are mainly discussing <em>data-driven</em> approaches here</li>

<li>Feature selection is <em>theory-driven</em>, usually</li>

</ul>

<p>Left off reading this:</p>

<p>https://books.google.com/books?hl=en&amp;lr=&amp;id=i8hQhp1a62UC&amp;oi=fnd&amp;pg=PT29&amp;dq=feature+extraction+encyclopedia+of+machine+learning&amp;ots=91mazyjCcQ&amp;sig=JVvFpdWbwrFQnAf737nzjkuCJjI#v=onepage&amp;q&amp;f=false</p>

</div><div id="section" class="slide section level2">

<h1>‚Ä¶</h1>

<p>Solves the curse üòà , but also presents a workaround to feature selection</p>

<ul>

<li>‚Ä¶</li>

</ul>

</div><div id="example" class="slide section level2">

<h1>Example</h1>

<p>A simple metaphor/image: ball on spring from Shlens?</p>

<p>[ball on spring]</p>

</div><div id="example-1" class="slide section level2">

<h1>Example</h1>

<p>Observation of this phenomenon: we don‚Äôt know the underlying system</p>

<p>[picture of ball on spring w/cameras]</p>

</div><div id="principal-component-analysis" class="slide section level2">

<h1>Principal component analysis</h1>

<p>A basic primer on how <strong>PCA</strong> ü§ñ works</p>

<ul>

<li>Eigenvectors: the latent dimensions discovered by PCA, which form a better basis for describing the data</li>

<li>Eigenvalues: the amount of variance in data set explained by each eigenvector</li>

<li>Projections (‚Äúscores‚Äù): transform of data into this new space, in terms of eigenvectors</li>

</ul>

</div><div id="eigendecomposition" class="slide section level2">

<h1>Eigendecomposition</h1>

<p>A complicated term for <strong>rotating</strong> high-dimensional data to find a new <strong>basis</strong> for describing the data</p>

<p>ü§ñ</p>

</div><div id="projection" class="slide section level2">

<h1>Projection</h1>

<p>The data can then be <strong>projected</strong> from its ‚Äúnaive‚Äù basis (the features originally measured) into a new basis</p>

<p>[figure or figures here]</p>

</div><div id="basic-example" class="slide section level2">

<h1>Basic example</h1>

<p>A basic example in NB üë©‚Äçüíªüßëüèæ‚Äçüíªüë©üèª‚Äçüíª</p>

<ul>

<li>Opening a sample data set</li>

<li>Eigendecomposition</li>

<li>Projection into new basis</li>

<li>Basic data viz</li>

</ul>

<p>‚Ä¶</p>

</div>

<div id="wrapping-up" class="title-slide slide section level1"><h1>Wrapping up</h1></div><div id="feature-extraction-advantages" class="slide section level2">

<h1>Feature extraction: advantages</h1>

<p>Speed, consistency/replicability, holistic description</p>

<p>Dimred can speed along discovery of new features to select</p>

</div><div id="dimensionality-reduction-advantages" class="slide section level2">

<h1>Dimensionality reduction: advantages</h1>

<p>More specifically:</p>

<ul>

<li>Aids data visualization, even occasionally rendering it in intuitive terms</li>

</ul>

</div><div id="feature-extraction-disadvantages" class="slide section level2">

<h1>Feature extraction: disadvantages</h1>

<p>New n-dimensional space may not be <em>interpretable</em>: what does a dimension ‚Äúmean‚Äù?</p>

<ul>

<li>Dimensions may not correspond to anything linguistically relevant</li>

</ul>

</div><div id="next-time" class="slide section level2">

<h1>Next time</h1>

</div><div id="references" class="slide section level2 bib">

<h1>References</h1>

<p>Aggarwal, C., Hinneburg, A. &amp; Keim, D. (2001). On the surprising behavior of distance metrics in high dimensional space. In <i>International Conference on Database Theory</i>, 420-434. Springer.</p>

<p>Bellman, R. (1957). <i>Dynamic Programming</i>. Princeton University Press.</p>

<p>Keogh, E. &amp; Mueen, A. (2011). Curse of dimensionality. In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 257-258. Springer.</p>

<p>Liu, H. (2011). Feature selection. In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 402-406. Springer.</p>

<p>Shlens, Y. (200x).</p>

<p>Styler, W. (2015). On the Acoustical and Perceptual Features of Vowel Nasality. PhD thesis. University of Colorado, Boulder. <a href="https://wstyler.ucsd.edu/files/styler_dissertation_final.pdf">PDF</a></p>

<p>Vlachos, M. (2011). Dimensionality reduction. In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 402-406. Springer.</p>

</div>

</body>

</html>
