<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"

 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

  <meta http-equiv="Content-Style-Type" content="text/css" />

  <meta name="generator" content="pandoc" />

  <meta name="author" content="Matthew Faytak University at Buffalo" />

  <title>NTU lectures (2)</title>

  <style type="text/css">

      code{white-space: pre-wrap;}

      span.smallcaps{font-variant: small-caps;}

      span.underline{text-decoration: underline;}

      div.column{display: inline-block; vertical-align: top; width: 50%;}

  </style>

  <link rel="stylesheet" type="text/css" media="screen, projection, print"

    href="assets/styles/slidy.css" />

  <script src="assets/scripts/slidy.js"

    charset="utf-8" type="text/javascript"></script>

</head>

<body>

<div class="slide titlepage">

  <h1 class="title">NTU lectures (2)</h1>

  <p class="author">

Matthew Faytak<br/>University at Buffalo

  </p>

  <p class="date"><img src="./assets/media/UB_Stacked_Small.png" width="200"> <img src="./assets/media/ntu-logo.png" width="200"><br/><img src="./assets/media/qr1.png" width="170"></p>

</div>

<div id="overview" class="slide section level2">

<h1>Overview</h1>

<p>In the previous lecture, we discussed <strong>feature selection</strong></p>

<ul>

<li>Manually <em>selecting</em> a subset of features <span class="cite">see Liu (2011)</span></li>

</ul>

<p>Here, we will discuss <strong>feature extraction</strong></p>

<ul>

<li>Especially using dimensionality reduction <span class="cite">see Vlachos (2011)</span></li>

<li>Taking a large number of features and using them to generate a smaller number of features</li>

<li>Reduces the number of features required to describe a data set</li>

</ul>

</div>

<div id="outlook" class="slide section level2">

<h1>Outlook</h1>

<p>Some different motivations underpin feature extraction: <strong>data-driven</strong> rather than theory-driven</p>

<ul>

<li>Allows new features well-suited to task to emerge from data</li>

<li>Mainly concerned with <em>performance</em> of feature as an input to some task</li>

<li>Especially frequent in <strong>machine learning</strong> and ‚Äúartificial intelligence‚Äù applications</li>

</ul>

</div>

<div id="dimensionality-reduction" class="title-slide slide section level1"><h1>Dimensionality reduction</h1></div><div id="features-as-dimensions" class="slide section level2">

<h1>Features as dimensions</h1>

<p>Features (observable attributes) define <strong>dimensions</strong> in a data set</p>

<ul>

<li>i.e.¬†the lower formants define a 3-dimensional space (F1, F2, F3)</li>

<li>3D vowel space of Kejom: <span class="cite">Faytak &amp; Akumbu (2021)</span></li>

</ul>

<p><img src="./assets/media/kejom_fig4.png" width="400"></p>

</div><div id="curse-of-dimensionality" class="slide section level2">

<h1>Curse of dimensionality</h1>

<p>If we have 1,000 features (dimensions) per observation, why not just use all of them? The <strong>curse of dimensionality</strong> üòà</p>

<p>Adding <em>many</em> dimensions exponentially increases the amount of space that needs to be covered, with numerous undesirable side effects <span class="cite">Bellman (1957); Keough &amp; Mueen (2011)</span></p>

<ul>

<li>Computation becomes exponentially more difficult</li>

<li>Exponentially more samples needed to evenly cover the space and maintain good performance</li>

</ul>

<p>Distance in very high-dimensional spaces becomes nonsensical <span class="cite">Aggarwal et al (2001)</span></p>

<ul>

<li>Objects become more or less equidistant from one another</li>

</ul>

</div><div id="dimensionality-reduction-1" class="slide section level2">

<h1>Dimensionality reduction</h1>

<p>Converts a high dimensional space consisting of numerous features into a lower dimensional space consisting of fewer features <span class="cite">Vlachos (2011)</span></p>

<p>Technically, feature selection is also a way of reducing dimensionality <span class="cite">Liu (2011), Vlachos (2011)</span></p>

<ul>

<li>Feature selection is <em>theory-driven</em>: we know what the informative dimensions are</li>

</ul>

<p>We are focusing today on <strong>feature projection</strong></p>

<ul>

<li>Finding a smaller number of dimensions to express data in</li>

<li><em>Data-driven</em> approach: we don‚Äôt know informative dimensions in advance; emerge from the data</li>

</ul>

</div><div id="benefits-of-dimensionality-reduction" class="slide section level2">

<h1>Benefits of dimensionality reduction</h1>

<p>Avoids high dimensionality üòà, and also presents a useful complement to feature selection</p>

<ul>

<li>Especially well suited to exploratory analysis</li>

<li>Or any other time when we don‚Äôt have background knowledge to perform feature selection</li>

</ul>

</div><div id="example" class="slide section level2">

<h1>Example</h1>

<p>Consider a ball on a spring <span class="cite">from Shlens (2014)</span></p>

<ul>

<li>Simple system - motion can be modeled as a sine wave-like time series along <span class="math inline"><em>x</em></span> dimension</li>

<li>If we don‚Äôt know what <span class="math inline"><em>x</em></span> is, though, we might place cameras at some arbitrary angles to try to find out</li>

</ul>

<p><img src="./assets/media/shlens1.png" width="500"></p>

</div><div id="example-1" class="slide section level2">

<h1>Example</h1>

<p>Measurement ends up being in terms of arbitrary dimensions which are <em>not the simplest, best way</em> to describe the system</p>

<ul>

<li>Six dimensions captured by our observations</li>

<li>Movement in <span class="math inline"><em>x</em>,‚ÄÜ<em>y</em></span> space filmed by cameras A, B, C <span class="cite">Shlens (2014)</span></li>

</ul>

<p><img src="./assets/media/shlens2.png" width="600"></p>

</div>

<div id="principal-component-analysis" class="title-slide slide section level1"><h1>Principal component analysis</h1></div><div id="principal-component-analysis-1" class="slide section level2">

<h1>Principal component analysis</h1>

<p>Data often contains lots of <strong>covariation</strong>: measures are not totally independent; can vary systematically together</p>

<ul>

<li>Like <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span> position in Camera A</li>

</ul>

<p><strong>PCA</strong> finds a new set of <em>orthogonal</em> dimensions which capture the most variance (and the least noise)</p>

<ul>

<li>The dimension shown below describes variation better than <span class="math inline"><em>x</em></span> or <span class="math inline"><em>y</em></span> of Camera A alone</li>

<li>Called <em>first principal component</em> or <strong>PC1</strong></li>

</ul>

<p><img src="./assets/media/shlens3.png" width="500"> <span class="cite"> Shlens (2014)</span></p>

</div><div id="pca-in-action" class="slide section level2">

<h1>PCA in action</h1>

<p>The previous slide showed PC1 for two-dimensional data; imagine PCA in many more dimensions, which is more typical</p>

<ul>

<li>PC-finding process can be repeated <span class="math inline"><em>n</em>‚ÄÖ‚àí‚ÄÖ1</span> times where <span class="math inline"><em>n</em></span> is the number of observations</li>

<li>When repeated <span class="math inline"><em>r</em></span> times, yields <span class="math inline"><em>r</em></span> dimensions <strong>PC1</strong>, <strong>PC2</strong>, <strong>PC3</strong>, ‚Ä¶ <strong>PC<span class="math inline"><em>r</em></span></strong></li>

<li>The most informative dimensions are discovered first</li>

</ul>

</div><div id="rotation" class="slide section level2">

<h1>Rotation</h1>

<p>New basis comes from <strong>rotation</strong> and rescaling of original data in high dimensions (see next slide)</p>

<p>PCs are a particular sort of <strong>eigenvector</strong></p>

<ul>

<li>The latent dimensions discovered by PCA</li>

<li>Form a better <strong>basis</strong> for describing the data</li>

</ul>

<p>The amount of variance in the original data explained by each eigenvector is its <strong>eigenvalue</strong></p>

<ul>

<li>Eigenvectors in PCA are rank-ordered according to their eigenvalues</li>

<li>Most explanatory (highest eigenvalues) first</li>

</ul>

</div><div id="projection" class="slide section level2">

<h1>Projection</h1>

<p>The data can then be <strong>projected</strong> from the ‚Äúnaive‚Äù basis (the features originally measured) into a new basis</p>

<ul>

<li>Characterizing each observation on the newly discovered dimensions</li>

<li>Often called PC ‚Äúscores‚Äù because of the sense that we are grading each observation</li>

</ul>

<p>Result: positions in new ‚ÄúPC space‚Äù</p>

<p><img src="./assets/media/rotation.png" width="600"></p>

</div><div id="the-math" class="slide section level2">

<h1>The math</h1>

<p>We‚Äôve skipped over the linear algebra that underpins PCA, for now</p>

<ul>

<li>Understanding the concepts is sufficient</li>

<li>We will return to a more mathematical definition later, when our data and our manipulations of it become more complicated</li>

</ul>

<p>Some further reading on the linear algebra:</p>

<ul>

<li>Shlens (2014), a brief overview including the ball-on-spring example</li>

<li>Jolliffe (2002), a reference book</li>

</ul>

<p>Other reading:</p>

<ul>

<li>Nguyen &amp; Holmes (2019), general tips</li>

</ul>

</div>

<div id="notebook-time" class="title-slide slide section level1"><h1>Notebook time</h1></div><div id="pca-in-python" class="slide section level2">

<h1>PCA in Python</h1>

<p>We now switch to our first Python notebook to demonstrate how to do PCA üë©‚Äçüíªüßëüèæ‚Äçüíªüë©üèª‚Äçüíª</p>

<ul>

<li>Opening a sample data set</li>

<li>Decomposition into eigenvectors</li>

<li>Projection of data into new basis</li>

<li>Basic data visualizations associated with PCA</li>

</ul>

<p>Using acoustic data on phonation from Keating et al.¬†(n.d.)</p>

</div>

<div id="wrapping-up" class="title-slide slide section level1"><h1>Wrapping up</h1></div><div id="feature-extraction-advantages" class="slide section level2">

<h1>Feature extraction: advantages</h1>

<p>Extracted/projected features can speed analysis</p>

<ul>

<li>Or make it clear which features are useful to select</li>

</ul>

<p>Improvements to research workflows</p>

<ul>

<li>Emerges <em>consistently</em> from given data set, and may improve replicability of analysis</li>

<li>Very <em>fast</em> compared to selection of some features</li>

</ul>

</div><div id="feature-extraction-advantages-1" class="slide section level2">

<h1>Feature extraction: advantages</h1>

<p>Dimensionality reduction can aid data visualization, sometimes even making it more intuitive</p>

<ul>

<li>PC1 in the ball-on-spring example basically describes motion along the spring, where neither <span class="math inline"><em>x</em></span> nor <span class="math inline"><em>y</em></span> do</li>

<li>Extremely useful at finding and capturing ‚Äúensembles‚Äù of features in higher-dimensional data

<ul>

<li>As in notebook: PCs capture covariation of groups of acoustic properties</li>

</ul></li>

</ul>

</div><div id="feature-extraction-disadvantages" class="slide section level2">

<h1>Feature extraction: disadvantages</h1>

<p>Not <em>theory-driven</em>, which may pose liabilities</p>

<ul>

<li>Science is primarily theory-driven</li>

<li>Engineering-type approach to non-engineering problems</li>

</ul>

<p>New <span class="math inline"><em>n</em></span>-dimensional space may not be so easy to interpret: what does a dimension ‚Äúmean‚Äù?</p>

<ul>

<li>Loadings may be difficult to interpret</li>

<li>Dimensions discovered in linguistic data <em>don‚Äôt have to</em> correspond to anything linguistically relevant</li>

</ul>

</div><div id="next-time" class="slide section level2">

<h1>Next time</h1>

<p>The second major thread of these lectures: ultrasound as a method</p>

<ul>

<li>Basic principles</li>

<li>Feature selection problems in ultrasound data</li>

<li>Other advances covered in subsequent lectures</li>

</ul>

</div><div id="references" class="slide section level2 bib">

<h1>References</h1>

<p>Aggarwal, C., Hinneburg, A. &amp; Keim, D. (2001). On the surprising behavior of distance metrics in high dimensional space. In <i>International Conference on Database Theory 2001</i>, 420-434. Springer. <a href="https://link.springer.com/chapter/10.1007/3-540-44503-X_27">PDF</a></p>

<p>Bellman, R. (1957). <i>Dynamic Programming</i>. Princeton University Press.</p>

<p>Faytak, M. &amp; Akumbu, P. (2021). Kejom (Babanki). <i>Journal of the International Phonetic Association</i>, 51(2), 333-354. <a href="https://doi.org/10.1017/S0025100319000264">DOI</a></p>

<p>Jolliffe, I. (2002). Principal component analysis (2nd ed.). <i>Springer series in statistics</i>. New York: Springer.</p>

<p>Keating, P., Kuang, J., Garellek, M., Esposito, C. &amp; Khan, S. (n.d.) A cross-language acoustic space for phonation distinctions. Unpublished manuscript, UCLA. <a href="https://linguistics.ucla.edu/people/keating/Keating-etal_2019_ms.pdf">PDF</a></p>

<p>Keogh, E. &amp; Mueen, A. (2011). Curse of dimensionality. In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 257-258. Springer.</p>

<p>Liu, H. (2011). Feature selection. In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 402-406. Springer.</p>

<p>Nguyen, L. &amp; Holmes, S. (2019). Ten quick tips for effective dimensionality reduction. <i>PLoS Computational Biology</i>, 15(6), e1006907. <a href="https://doi.org/10.1371/journal.pcbi.1006907">DOI</a></p>

<p>Shlens, J. (2014). A Tutorial on Principal Component Analysis. <a href="https://arxiv.org/abs/1404.1100">arXiv</a></p>

<p>Vlachos, M. (2011). Dimensionality reduction. In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 402-406. Springer.</p>

<p>Zhang, X. (2011). Covariance matrix. In In Sammut, C. &amp; Webb, G. (eds.), <i>Encyclopedia of Machine Learning</i>, 235-238. Springer.</p>

</div>

</body>

</html>
