<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"

 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

  <meta http-equiv="Content-Style-Type" content="text/css" />

  <meta name="generator" content="pandoc" />

  <meta name="author" content="Matthew Faytak University at Buffalo NTU lecture series" />

  <title>“Pixel methods” for ultrasound</title>

  <style type="text/css">

      code{white-space: pre-wrap;}

      span.smallcaps{font-variant: small-caps;}

      span.underline{text-decoration: underline;}

      div.column{display: inline-block; vertical-align: top; width: 50%;}

  </style>

  <link rel="stylesheet" type="text/css" media="screen, projection, print"

    href="assets/styles/slidy.css" />

  <script src="assets/scripts/slidy.js"

    charset="utf-8" type="text/javascript"></script>

</head>

<body>

<div class="slide titlepage">

  <h1 class="title">“Pixel methods” for ultrasound</h1>

  <p class="author">

Matthew Faytak<br/>University at Buffalo<br/>NTU lecture series

  </p>

  <p class="date"><img src="./assets/media/UB_Stacked_Small.png" width="200"> <img src="./assets/media/ntu-logo.png" width="200"><br/><img src="./assets/media/qr1.png" width="170"></p>

</div>

<div id="refresher-contour-extraction" class="slide section level2">

<h1>Refresher: contour extraction</h1>

<p>Contours can be extracted from the ultrasound image using a combination of human-generated hints and automatic processing <span class="cite">Iskarous (2005); Stone (2005)</span></p>

<p><img src="./assets/media/goat-big.png" width="500"></p>

</div>

<div id="refresher-ultrasound-data-analysis" class="slide section level2">

<h1>Refresher: ultrasound data analysis</h1>

<p>Usually done on <strong>contours</strong> with a spline model which can handle non-linear patterns (like tongue shapes) <span class="cite">Davidson (2006); Heyne et al (2019)</span></p>

<ul>

<li>SSANOVA (smoothing spline ANOVA), pictured below</li>

<li>GAMMs</li>

<li>Both quite computationally intensive</li>

</ul>

<p><img src="./assets/media/weller-et-al.png" width="650"></p>

<p><span class="cite">figure from Weller et al. (to appear)</span></p>

</div>

<div id="overview-this-lecture" class="slide section level2">

<h1>Overview: this lecture</h1>

<p>Various types of <strong>feature engineering</strong></p>

<ul>

<li>Generating new features from existing ones</li>

<li>Often through recombination, averaging, etc.</li>

</ul>

<p>All get around feature extraction and the need for (most) human intervention by focusing on image’s <strong>pixels</strong> as a set of features</p>

<ul>

<li>Pixel difference methods</li>

<li>Optical flow</li>

<li>Dimensionality reduction on ultrasound frames

<ul>

<li>Our focus here and in our final notebook</li>

</ul></li>

</ul>

</div>

<div id="pixel-based-motion-detection" class="title-slide slide section level1"><h1>Pixel-based motion detection</h1></div><div id="pixels" class="slide section level2">

<h1>Pixels</h1>

<p>Each ultrasound image is composed of tens of thousands of pixels, each of which has a numerical value indicating brightness</p>

<p><img src="./assets/media/ultrasound-pixels2.png" width="300"> <img src="./assets/media/ultrasound-pixels-zoom.png" width="200"></p>

<ul>

<li>Directly relates to position of tongue: brightness means reflectivity</li>

</ul>

</div><div id="pixel-shape" class="slide section level2">

<h1>Pixel shape</h1>

<p>The pixels are <em>arc-shaped</em> in most ultrasound frames because the raw reflection data is stored as a rectangular grid <span class="cite">Wrench &amp; Scobbie (2008); Eshky et al (2021)</span></p>

<ul>

<li>Column = <strong>scan line</strong> (the energy/reflection of one element in probe)</li>

<li>Row = distance from probe</li>

<li>Color = reflectivity</li>

</ul>

<p>This grid is transformed to real-world proportions before we work with it <span class="cite">figure from Eshky et al (2021)</span></p>

<p><img src="./assets/media/eshky.jpg" width="600"></p>

</div><div id="pixel-difference-pd" class="slide section level2">

<h1>Pixel difference (PD)</h1>

<p>Tongue position <em>change</em> means pixels change in brightness from frame to frame</p>

<ul>

<li>Some pixels gain brightness as tongue moves into their region</li>

<li>Others lose brightness as tongue moves away</li>

</ul>

<p>The <strong>Euclidean distance</strong> of two frames in terms of all their pixels can be used as a measure of tongue movement <span class="cite">figure from Palo (2019)</span></p>

<p><img src="./assets/media/palo-lemon.png" width="600"></p>

</div><div id="definition" class="slide section level2">

<h1>Definition</h1>

<p>Defining PD more precisely <span class="cite">Palo (2019)</span>:</p>

<ul>

<li>Each frame <span class="math inline"><em>F</em></span> is an <span class="math inline"><em>n</em><sub><em>x</em></sub> × <em>n</em><sub><em>y</em></sub></span> dimensional vector, where <span class="math inline"><em>n</em><sub><em>x</em></sub></span> is scanlines and <span class="math inline"><em>n</em><sub><em>y</em></sub></span> is pixels per scanline</li>

</ul>

<p><img src="./assets/media/palo-eqn1.png" width="450"></p>

<ul>

<li>For <span class="math inline"><em>k</em> = {<em>F</em><sub>1</sub>, <em>F</em><sub>2</sub>, …<em>F</em><sub><em>n</em> − 1</sub>}</span></li>

</ul>

</div><div id="step-size" class="slide section level2">

<h1>Step size</h1>

<p>We can calculate the difference over successive frames (step size 1), or over frames more separated in time (step size <span class="math inline"><em>L</em></span>)</p>

<p><img src="./assets/media/palo-eqn2.png" width="425"></p>

<ul>

<li><p>for <span class="math inline"><em>k</em> = {<em>F</em><sub>1</sub>, <em>F</em><sub>2</sub>, …<em>F</em><sub><em>n</em> − <em>L</em></sub>}</span></p></li>

<li><p>The time associated with this measurement is the average of the time of the two involved frames, or <span class="math inline">1/2(<em>t</em><sub><em>F</em><sub><em>k</em></sub></sub> + <em>t</em><sub><em>F</em><sub><em>k</em> + <em>L</em></sub></sub>)</span></p></li>

</ul>

<p>Palo (2019) mainly uses a step size of 1</p>

<ul>

<li>Though depending on the articulation at issue, larger step sizes may be appropriate</li>

</ul>

</div><div id="applications-of-pd" class="slide section level2">

<h1>Applications of PD</h1>

<p>Movement detected includes <em>intrinsic tongue muscles</em>, unlike other measures discussed so far</p>

<ul>

<li>Measures more than just movement in the surface contour</li>

<li>Can even detect a speaker’s heart rate (pulse in vessels in the tongue)</li>

</ul>

<p>Various psycholinguistic applications <span class="cite">McMillan &amp; Corley (2010); Palo (2019)</span></p>

<ul>

<li>Useful for detection of pre-speech articulation which may not be represented in the acoustic signal</li>

<li>Refinement of reaction times based on production</li>

</ul>

</div><div id="optical-flow-of" class="slide section level2">

<h1>Optical flow (OF)</h1>

<p>Related but more computationally complex: <strong>optical flow</strong>, which detects apparent motion between two frames <span class="cite">Horn &amp; Schunck (1981)</span></p>

<ul>

<li>Like PD, detects magnitude of motion; unlike PD, gives <em>direction</em> of motion as well</li>

<li>Produces a <strong>flow field</strong> for smaller elements in image, which can be averaged to a <strong>consensus</strong> <span class="cite">figure from Faytak, Moisik, &amp; Palo (2021)</span></li>

</ul>

<p><img src="./assets/media/satkit-of.png" width="450"></p>

</div><div id="applications-of-of" class="slide section level2">

<h1>Applications of OF</h1>

<p>Integrating the velocity signal gives amount of displacement for rigid bodies (i.e. the larynx) <span class="cite"> Moisik et al (2014); figure from Faytak, Moisik &amp; Palo (2021)</span></p>

<p><img src="./assets/media/satkit-movement.png" width="500"></p>

</div>

<div id="pixel-dimensionality-reduction" class="title-slide slide section level1"><h1>Pixel dimensionality reduction</h1></div><div id="pixel-dimensionality-reduction-1" class="slide section level2">

<h1>Pixel dimensionality reduction</h1>

<p>Dimensionality reduction carried out not on ultrasound contours, but on entire <strong>ultrasound frames</strong></p>

<ul>

<li>Ultrasound frames are made up of tens of thousands of individual pixels</li>

<li>Each with a value indicating brightness (e.g. 0=black, 255=white)</li>

</ul>

<p>Brightness values can be used as raw data for dimensionality reduction</p>

</div><div id="pixels-and-scan-lines" class="slide section level2">

<h1>Pixels and scan lines</h1>

<p>Each ultrasound frame can be thought of as a matrix of width <span class="math inline"><em>w</em></span> by height <span class="math inline"><em>h</em></span></p>

<ul>

<li><span class="math inline"><em>w</em>, <em>h</em></span> values must be fixed throughout data collection (images must be same size)</li>

<li>Each column of pixels of shape <span class="math inline">1 × <em>h</em></span> represents a single <strong>scan line</strong> from the probe</li>

<li>Each scan line is reflectivity data from a single element in the probe</li>

</ul>

<p><img src="./assets/media/scanlines.jpg" width="375"></p>

</div><div id="high-dimensionality" class="slide section level2">

<h1>High dimensionality</h1>

<p>Each pixel at location <span class="math inline"><em>x</em>, <em>y</em></span> across data sets with the same frame size <span class="math inline"><em>w</em> × <em>h</em></span> can be thought of as a separate feature</p>

<ul>

<li>Meaning: each frame contains thousands, or tens of thousands, of features</li>

</ul>

<p>Challenge for <strong>feature selection</strong>:</p>

<ul>

<li>Bad idea to use every single pixel (curse of dimensionality)</li>

<li>Not easy to pick a small number of pixels which are good predictors of the phenomenon we’re studying</li>

<li>Solution: <strong>engineer new features</strong> which capture the variation</li>

</ul>

</div><div id="recap-dimensionality-reduction" class="slide section level2">

<h1>Recap: dimensionality reduction</h1>

<p>PCA outputs <strong>eigenvectors</strong> and <strong>eigenvalues</strong></p>

<ul>

<li>Eigenvectors (PCs) are patterns of variation uncovered in the data</li>

<li>Eigenvalues assign importance to explaining variation in the data</li>

<li>Observations can be transformed from naive basis into “PC space”</li>

</ul>

<p>We might recall this was applied to numerous acoustic measures in our first notebook</p>

<p>Eigenvector loadings:</p>

<p><img src="./assets/media/eigenvector-basic.png" width="550"></p>

<p>Data in PC space:</p>

<p><img src="./assets/media/notebook1-recap.png" width="600"></p>

</div><div id="calculating-image-eigenvectors" class="slide section level2">

<h1>Calculating image eigenvectors</h1>

<p>Method can be extended to <em>image data</em>: pixels in an image of shape <span class="math inline"><em>w</em> × <em>h</em></span> are treated as a <em>very long list</em> of features of length <span class="math inline"><em>w</em><em>h</em></span></p>

<ul>

<li>Basically, rows or columns are “unstacked” and “put end to end” to form a list</li>

</ul>

<p><img src="./assets/media/frametolist.png" width="600"></p>

<p>Like in the notebook’s dataframe:</p>

<ul>

<li>Each row in the dataframe is an observation (i.e. an ultrasound frame)</li>

<li>Each column in the dataframe is a feature (pixel in a given location)</li>

<li>The difference: <em>many more</em> columns/features</li>

</ul>

<p><img src="./assets/media/flatten.png" width="600"></p>

</div><div id="eigenvectors-as-eigenpictures" class="slide section level2">

<h1>Eigenvectors as eigenpictures</h1>

<p>Convert our length <span class="math inline"><em>w</em><em>h</em></span> eigenvectors back to <span class="math inline"><em>w</em> × <em>h</em></span> grids to get impact of associated PCs on pixel brightness <em>in physical space</em> <span class="cite">Sirovich &amp; Kirby (1987)</span></p>

<ul>

<li>Help us to evaluate what the discovered dimensions are</li>

<li>Famously applied to faces; eigenvectors converted back to original dimensions are called <strong>eigenfaces</strong> <span class="cite">Turk &amp; Pentland (1991)</span></li>

<li>Also applied to images of lips alone (eigenlips) <span class="cite">Bregler &amp; Konig (1994)</span></li>

</ul>

<p>We could reshape our notebook’s eigenvectors, but because feature number doesn’t correspond to physical space, it doesn’t really gain us anything</p>

<p><img src="./assets/media/reshape-basic.png" width="600"></p>

</div><div id="eigenfaces" class="slide section level2">

<h1>Eigenfaces</h1>

<p>When the features correspond to physical position (i.e. of pixels), eigenpictures tell us more: <span class="cite">figures from Turk &amp; Pentland (1991)</span></p>

<ul>

<li>We used red/blue diverging color scale in notebook; here white = positive, black = negative</li>

</ul>

<p><img src="./assets/media/turk-rotation.png" width="700"></p>

<p>Eigenfaces show variation in shape and size of facial features, hair, and setting (lighting, pose, etc)</p>

<ul>

<li>The most informative eigenfaces (PCs) mainly capture variation in hair and facial hair</li>

<li>Along with glasses, lighting, and probably skin tone</li>

</ul>

</div><div id="eigentongues" class="slide section level2">

<h1>Eigentongues</h1>

<p>Hueber et al (2007) coined <strong>eigentongues</strong>, from eigenfaces</p>

<ul>

<li>Take the PC loadings obtained from our “flattened” frame data, and “unflatten” them</li>

</ul>

<p><img src="./assets/media/eigentongue-basic2.png" width="700"></p>

<ul>

<li>Covariation of pixel brightness with PCs shows variation in <em>tongue contour</em> position

<ul>

<li>And any other patterns in data set (hyoid shadow position, internal musculature of tongue, …)</li>

</ul></li>

<li>Here, higher PC1 “moves” upper part of contour to the right; lower PC1 “moves” it to the left</li>

</ul>

</div><div id="eigentongues-1" class="slide section level2">

<h1>Eigentongues</h1>

<p>Another example, from <em>all</em> tongue postures in a corpus, at a lower spatial resolution <span class="cite">figures from Hueber et al (2007)</span></p>

<ul>

<li>Again, white = positive covariation with PC score; black = negative</li>

<li>Higher PC1 score makes brighter pixels light up</li>

<li>Lower PC1 score makes darker pixels light up</li>

</ul>

<p><img src="./assets/media/hueber-all.png" width="700"></p>

</div><div id="pc-scores" class="slide section level2">

<h1>PC scores</h1>

<p>We can now characterize our ultrasound image data set in terms of the <strong>PC scores</strong> for each eigentongue</p>

<ul>

<li>Here, PC1 clearly identifies two clusters, each of which varies in PC2 and PC3</li>

</ul>

<p><img src="./assets/media/eigen-pcs.png" width="600"></p>

</div>

<div id="applications-of-eigentongues" class="title-slide slide section level1"><h1>Applications of eigentongues</h1></div><div id="feature-engineering" class="slide section level2">

<h1>Feature engineering</h1>

<p>On a practical level, avoids <strong>feature selection</strong> problems by making new ones; avoids time-consuming process of <strong>contour extraction</strong></p>

<ul>

<li>Eigentongues capture informative variation across the set of images</li>

<li>Most often, this is linguistically interesting (because most tongue motions have linguistic consequences)</li>

</ul>

<p>We can also do some neat tricks with eigentongues which aren’t easy with other approaches</p>

<ul>

<li>Reviewed in our final notebook</li>

<li>… but first, an overview here</li>

</ul>

</div><div id="time-series-of-pc-scores" class="slide section level2">

<h1>Time series of PC scores</h1>

<p><em>Sequences of frames</em> can be fed to PCA instead of frames at a single point of interest (i.e. midpoint); yields <strong>time series</strong> data <span class="cite">Mielke et al (2017); Hoole &amp; Pouplier (2017); Smith et al (2019)</span></p>

<ul>

<li>Here, simple case of PC1 capturing /i/ with high scores and /a/ at low scores <span class="cite">figure from Hoole &amp; Pouplier (2017)</span></li>

</ul>

<p><img src="./assets/media/hoole-pouplier.png" width="600"></p>

<ul>

<li>“Front-raising” coarticulation over time represented by full range of scores</li>

</ul>

</div><div id="linear-discriminant-analysis" class="slide section level2">

<h1>Linear discriminant analysis</h1>

<p>Another dimensionality-reduction technique which eigentongue PC scores can be submitted to <span class="cite">see Carignan (2019) </span></p>

<ul>

<li>Learns new dimension(s) which maximizes separation of labeled categories</li>

<li>LDA can be used as “segment detector” (i.e. discriminate /r/ vs. all other segments) <span class="cite">Mielke et al (2017); figure from Smith et al (2019)</span></li>

</ul>

<p><img src="./assets/media/smith-etal.png" width="600"></p>

<ul>

<li>Can also be used to determine how consistent or discrete a contrast is (i.e. [l] vs. [lˠ]; /n/ vs. /ŋ/) <span class="cite">Strycharczuk &amp; Scobbie (2017); Faytak et al (2020)</span></li>

</ul>

</div><div id="reconstruction" class="slide section level2">

<h1>Reconstruction</h1>

<p>Weighted combinations of eigentongues can <strong>reconstruct</strong> observations <span class="cite">Hoole &amp; Pouplier (2017); Faytak et al. (2020)</span></p>

<p>Specifically, an image <span class="math inline"><em>Γ</em></span> can be reconstructed as linear combination of <span class="math inline"><em>m</em></span> eigentongues: <span class="cite">Berry (2012)</span></p>

<p><img src="./assets/media/berry-math.png" width="300"></p>

<p>where <span class="math inline"><em>u</em><sub><em>m</em></sub></span> is the <span class="math inline"><em>m</em></span>th eigentongue and <span class="math inline"><em>ω</em><sub><em>m</em></sub></span> is the <strong>projection</strong> of <span class="math inline"><em>Γ</em></span> onto the <span class="math inline"><em>m</em></span>th eigentongue (i.e. PC score)</p>

</div><div id="reconstruction-1" class="slide section level2">

<h1>Reconstruction</h1>

<p>Creates a <em>denoised</em> version of the observation <span class="cite">figures from Faytak et al. (2020)</span></p>

<p><img src="./assets/media/f-etal-raw.png" width="600"></p>

<p><img src="./assets/media/f-etal-eigen.png" width="400"></p>

<p><img src="./assets/media/f-etal-recon.png" width="450"></p>

</div>

<div id="wrapping-up" class="title-slide slide section level1"><h1>Wrapping up</h1></div><div id="pixel-methods-pros" class="slide section level2">

<h1>Pixel methods: pros</h1>

<p>Very efficient once the basics are mastered</p>

<ul>

<li>Speedy (big advantage over basic contour extraction) and replicable</li>

<li>Avoids feature selection by engineering new ones</li>

<li>Takes information into account beyond tongue surface</li>

</ul>

<p>Pixel methods easy to use on other data types</p>

<ul>

<li>MRI <span class="cite">Oh &amp; Lee (2018)</span></li>

<li>Video of face, especially lips <span class="cite">Krause et al (2020)</span></li>

</ul>

</div><div id="pixel-methods-cons" class="slide section level2">

<h1>Pixel methods: cons</h1>

<p>Fairly different from some approaches to analysis</p>

<ul>

<li>More computationally involved than feature-selection methods</li>

<li>Eigentongue methods primarily measure similarity and difference</li>

</ul>

<p>Eigentongue analysis only works properly within single speakers</p>

<ul>

<li>Size, frame of reference will bleed into PCs</li>

<li>No automatic separation of linguistic and non-linguistic variation</li>

</ul>

</div><div id="up-next-second-notebook" class="slide section level2">

<h1>Up next: second notebook</h1>

<p>Our final lecture will cover a Python implementation of eigentongue methods from my recent work</p>

<ul>

<li>Image preprocessing</li>

<li>Carrying out PCA</li>

<li>Interpreting eigentongues</li>

<li>Using eigentongues for:</li>

<li>Reconstruction of (mean) grouped data</li>

<li>Linear discriminant analysis (LDA)</li>

</ul>

<p>If you are curious about how to implement pixel difference or optical flow in Python:</p>

<ul>

<li>See the <a href="https://github.com/giuthas/satkit">SATKit</a> repo, which currently supports these methods <span class="cite">(Palo et al. (2022)</span></li>

<li>Faytak, Moisik &amp; Palo (2021) describes planned capabilities</li>

</ul>

</div><div id="references" class="slide section level2 bib">

<h1>References</h1>

<p>Berry, J. (2012). Machine learning methods for articulatory data. Doctoral dissertation, University of Arizona. <a href="https://www.proquest.com/docview/1013994476">PDF</a></p>

<p>Bregler, C. &amp; Konig, Y. (1994). “Eigenlips” for robust speech recognition. In <em>Proceedings of ICASSP ’94 Vol. 2</em>. <a href="https://doi.org/10.1109/ICASSP.1994.389567">DOI</a></p>

<p>Danilouchkine, M., Mastik, F. &amp; van der Steen, A. (2009). A study of coronary artery rotational motion with dense scale-space optical flow in intravascular ultrasound. <em>Physics in Medicine and Biology</em>, 54(6), 1397–1418. <a href="https://doi.org/10.1088/0031-9155/54/6/002">DOI</a></p>

<p>Carignan, C. (2019). TRACTUS (Temporally Resolved Articulatory Configuration Tracking of Ultrasound). Software. <a href="https://github.com/ChristopherCarignan/TRACTUS">GitHub</a></p>

<p>Davidson, L. (2006). Comparing tongue shapes from ultrasound imaging using smoothing spline analysis of variance. <em>The Journal of the Acoustical Society of America</em>, 120, pp. 407–415. <a href="https://doi.org/10.1121/1.2205133">DOI</a></p>

<p>Eshky, A., Cleland, J., Ribeiro, M., Sugden, E., Richmond, K. &amp; Renals, S. (2021). Automatic audiovisual synchronisation for ultrasound tongue imaging. <em>Speech Communication</em>, 132, 83-95. <a href="https://doi.org/10.1016/j.specom.2021.05.008">DOI</a></p>

<p>Faytak, M., Moisik, S. &amp; Palo, P. (2021). The Speech Articulation Toolkit (SATKit): Ultrasound image analysis in Python. In <em>Proceedings of ISSP 12</em>, 234-237. <a href="https://issp2020.yale.edu/ProcISSP2020.pdf">PDF</a></p>

<p>Faytak, M., Liu, S. &amp; Sundara, M. (2020). Nasal coda neutralization in Shanghai Mandarin: Articulatory and perceptual evidence. <em>Laboratory Phonology</em>, 11(1), 23. <a href="https://doi.org/10.5334/labphon.269">DOI</a></p>

<p>Heyne, M., Derrick, D., &amp; Al-Tamimi, J. (2019). Native language influence on brass instrument performance: An application of generalized additive mixed models (GAMMs) to midsagittal ultrasound images of the tongue. <em>Frontiers in Psychology</em>, 2597. <a href="https://doi.org/10.3389/fpsyg.2019.02597">DOI</a></p>

<p>Hoole, P. &amp; Pouplier, M. (2017). Öhman returns: New horizons in the collection and analysis of imaging data in speech production research. <em>Computer Speech &amp; Language</em>, 45, 253-277. <a href="https://doi.org/10.1016/j.csl.2017.03.002">DOI</a></p>

<p>Horn, B., &amp; Schunck, B. (1981). Determining optical flow. <em>Artificial Intelligence</em>, 17(1), 185–203. <a href="https://doi.org/10.1016/0004-3702(81)90024-2">DOI</a></p>

<p>Hueber, T., Aversano, G., Chollet, G., Denby, B., Dreyfus, G., Oussar, Y., Roussel, P. &amp; Stone, M. (2007). Eigentongue feature extraction for an ultrasound-based silent speech interface. In <em>Proceedings of ICASSP ’07 Vol. 1</em>. <a href="https://doi.org/10.1109/ICASSP.2007.366140">DOI</a></p>

<p>Iskarous, K. (2005). Detecting the edge of the tongue: A tutorial. <em>Clinical Linguistics &amp; Phonetics</em>, 19(6-7), 555-565. <a href="https://doi.org/10.1080/02699200500113871">DOI</a></p>

<p>Krause, P., Kay, C. &amp; Kawamoto, A., (2020) Automatic motion tracking of lips using digital video and OpenFace 2.0, <em>Laboratory Phonology</em> 11(1), 9. <a href="https://doi.org/10.5334/labphon.232">DOI</a></p>

<p>McMillan, C. &amp; Corley, M. (2010). Cascading influences on the production of speech: Evidence from articulation. <em>Cognition</em>, 117(3), 243–260. <a href="https://doi.org/10.1016/j.cognition.2010.08.019">DOI</a></p>

<p>Mielke, J., Carignan, C. &amp; Thomas, E. (2017). The articulatory dynamics of pre-velar and pre-nasal /æ/-raising in English: An ultrasound study. <em>The Journal of the Acoustical Society of America</em>, 142(1), 332-349. <a href="https://doi.org/10.1121/1.4991348">DOI</a></p>

<p>Moisik, S., Lin, H., &amp; Esling, J. (2014). A study of laryngeal gestures in Mandarin citation tones using simultaneous laryngoscopy and laryngeal ultrasound (SLLUS). <em>JIPA</em>, 44(1), 21– 58. <a href="https://doi.org/10.1017/S0025100313000327">DOI</a></p>

<p>Oh, M., &amp; Lee, Y. (2018). ACT: An Automatic Centroid Tracking tool for analyzing vocal tract actions in real-time magnetic resonance imaging speech production data. <em>The Journal of the Acoustical Society of America</em>, 144(4), EL290-EL296. <a href="https://doi.org/10.1121/1.5057367">DOI</a></p>

<p>Palo, P. (2019). Measuring pre-speech articulation. Doctoral dissertation, Queen Margaret University. <a href="https://eresearch.qmu.ac.uk/handle/20.500.12289/10163">PDF</a></p>

<p>Palo, P., Moisik, S. &amp; Faytak, M. (2022). Speech Articulation Toolkit (SATKit). Software. <a href="https://github.com/giuthas/satkit">GitHub</a></p>

<p>Smith, B., Mielke, J., Magloughlin, L. &amp; Wilbanks, E. (2019) Sound change and coarticulatory variability involving English /ɹ/. <em>Glossa: A Journal of General Linguistics</em> 4(1), 63. <a href="https://doi.org/10.5334/gjgl.650">DOI</a></p>

<p>Stone, M. (2005). A guide to analysing tongue motion from ultrasound images. <em>Clinical Linguistics &amp; Phonetics</em>, 19(6-7), 455-501. <a href="https://doi.org/10.1080/02699200500113558">DOI</a></p>

<p>Strycharczuk, P. &amp; Scobbie, J. (2017). Whence the fuzziness? Morphological effects in interacting sound changes in Southern British English. <em>Laboratory Phonology</em> 8(1), 7. <a href="http://doi.org/10.5334/labphon.24">DOI</a></p>

<p>Turk, M. &amp; Pentland, A. (1991). Eigenfaces for recognition. <em>Journal of Cognitive Neuroscience</em>, 3(1), 71-86. <a href="https://doi.org/10.1162/jocn.1991.3.1.71">DOI</a></p>

<p>Weller, J., Faytak, M., Steffman, J., Mayer, C., Teixeira, G. &amp; Tankou, R. (to appear). Supralaryngeal articulation across voicing and aspiration in Yemba vowels. In <em>Proceedings of ACAL 51/52</em>.</p>

<p>Wrench, A., &amp; Scobbie, J. (2008). High-speed cineloop ultrasound vs. video ultrasound tongue imaging: Comparison of front and back lingual gesture location and relative timing. In <em>Proceedings of ISSP 8</em>. <a href="https://eresearch.qmu.ac.uk/handle/20.500.12289/2012">PDF</a></p>

</div>

</body>

</html>
