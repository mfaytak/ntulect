<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"

 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

  <meta http-equiv="Content-Style-Type" content="text/css" />

  <meta name="generator" content="pandoc" />

  <meta name="author" content="Matthew Faytak University at Buffalo" />

  <title>NTU lectures (4)</title>

  <style type="text/css">

    code{white-space: pre-wrap;}

    span.smallcaps{font-variant: small-caps;}

    span.underline{text-decoration: underline;}

    div.column{display: inline-block; vertical-align: top; width: 50%;}

    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

    ul.task-list{list-style: none;}

    .display.math{display: block; text-align: center; margin: 0.5rem auto;}

  </style>

  <link rel="stylesheet" type="text/css" media="screen, projection, print"

    href="assets/styles/slidy.css" />

  <script src="assets/scripts/slidy.js"

    charset="utf-8" type="text/javascript"></script>

</head>

<body>

<div class="slide titlepage">

  <h1 class="title">NTU lectures (4)</h1>

  <p class="author">

Matthew Faytak<br/>University at Buffalo

  </p>

  <p class="date"><img src="./assets/media/UB_Stacked_Small.png" width="200"> <img src="./assets/media/ntu-logo.png" width="200"><br/><img src="./assets/media/qr1.png" width="170"></p>

</div>

<div id="refresher-contour-extraction" class="slide section level2">

<h1>Refresher: contour extraction</h1>

<p>Contours can be extracted from the ultrasound image using a combination of human-generated hints and automatic processing <span class="cite">Iskarous (2005); Stone (2005)</span></p>

<p><img src="./assets/media/goat-big.png" width="500"></p>

</div>

<div id="refresher-ultrasound-data-analysis" class="slide section level2">

<h1>Refresher: ultrasound data analysis</h1>

<p>Usually done on <strong>contours</strong> with a spline model which can handle non-linear patterns (like tongue shapes) <span class="cite">Davidson (2006); Heyne et al (2019)</span></p>

<ul>

<li>SSANOVA (smoothing spline ANOVA), pictured below</li>

<li>GAMMs</li>

<li>Both quite computationally intensive</li>

</ul>

<p><img src="./assets/media/weller-et-al.png" width="650"></p>

<p><span class="cite">figure from Weller et al. (to appear)</span></p>

</div>

<div id="overview-this-lecture" class="slide section level2">

<h1>Overview: this lecture</h1>

<p>Various types of <strong>feature engineering</strong></p>

<ul>

<li>Generating new features from existing ones</li>

<li>Often through recombination, averaging, etc.</li>

</ul>

<p>All get around feature extraction and the need for (most) human intervention</p>

<ul>

<li>Pixel difference methods</li>

<li>Optical flow</li>

<li>Dimensionality reduction on ultrasound frames

<ul>

<li>Our focus here and in our final notebook</li>

</ul></li>

</ul>

</div>

<div id="pixel-based-motion-detection" class="title-slide slide section level1">

<h1>Pixel-based motion detection</h1>



</div>

<div id="pixels" class="slide section level2">

<h1>Pixels</h1>

<p>Each ultrasound image is composed of tens of thousands of pixels, each of which has a numerical value indicating brightness</p>

<p><img src="./assets/media/ultrasound-pixels2.png" width="300"> <img src="./assets/media/ultrasound-pixels-zoom.png" width="200"></p>

<ul>

<li>Directly relates to position of tongue: brightness means reflectivity</li>

</ul>

</div>

<div id="pixel-shape" class="slide section level2">

<h1>Pixel shape</h1>

<p>The pixels are <em>arc-shaped</em> in most ultrasound frames because the raw reflection data is stored as a rectangular grid <span class="cite">Wrench &amp; Scobbie (2008); Eshky et al (2021)</span></p>

<ul>

<li>Column = <strong>scan line</strong> (the energy/reflection of one element in probe)</li>

<li>Row = distance from probe</li>

<li>Color = reflectivity</li>

</ul>

<p>This grid is transformed to real-world proportions before we work with it <span class="cite">figure from Eshky et al (2021)</span></p>

<p><img src="./assets/media/eshky.jpg" width="600"></p>

</div>

<div id="pixel-difference-pd" class="slide section level2">

<h1>Pixel difference (PD)</h1>

<p>Tongue position <em>change</em> means pixels change in brightness from frame to frame</p>

<ul>

<li>Some pixels gain brightness as tongue moves into their region</li>

<li>Others lose brightness as tongue moves away</li>

</ul>

<p>The <strong>Euclidean distance</strong> of two frames in terms of all their pixels can be used as a measure of tongue movement <span class="cite">figure from Palo (2019)</span></p>

<p><img src="./assets/media/palo-lemon.png" width="600"></p>

</div>

<div id="definition" class="slide section level2">

<h1>Definition</h1>

<p>Defining PD more precisely <span class="cite">Palo (2019)</span>:</p>

<ul>

<li>Each frame <span class="math inline"><em>F</em></span> is an <span class="math inline"><em>n</em><sub><em>x</em></sub> × <em>n</em><sub><em>y</em></sub></span> dimensional vector, where <span class="math inline"><em>n</em><sub><em>x</em></sub></span> is scanlines and <span class="math inline"><em>n</em><sub><em>y</em></sub></span> is pixels per scanline</li>

</ul>

<p><img src="./assets/media/palo-eqn1.png" width="450"></p>

<ul>

<li>For <span class="math inline"><em>k</em> = {<em>F</em><sub>1</sub>, <em>F</em><sub>2</sub>, …<em>F</em><sub><em>n</em> − 1</sub>}</span></li>

</ul>

</div>

<div id="step-size" class="slide section level2">

<h1>Step size</h1>

<p>We can calculate the difference over successive frames (step size 1), or over frames more separated in time (step size <span class="math inline"><em>L</em></span>)</p>

<p><img src="./assets/media/palo-eqn2.png" width="425"></p>

<ul>

<li><p>for <span class="math inline"><em>k</em> = {<em>F</em><sub>1</sub>, <em>F</em><sub>2</sub>, …<em>F</em><sub><em>n</em> − <em>L</em></sub>}</span></p></li>

<li><p>The time associated with this measurement is the average of the time of the two involved frames, or <span class="math inline">1/2(<em>t</em><sub><em>F</em><sub><em>k</em></sub></sub>+<em>t</em><sub><em>F</em><sub><em>k</em> + <em>L</em></sub></sub>)</span></p></li>

</ul>

<p>Palo (2019) mainly uses a step size of 1</p>

<ul>

<li>Though depending on the articulation at issue, larger step sizes may be appropriate</li>

</ul>

</div>

<div id="applications-of-pd" class="slide section level2">

<h1>Applications of PD</h1>

<p>Movement detected includes <em>intrinsic tongue muscles</em>, unlike other measures discussed so far</p>

<ul>

<li>Measures more than just movement in the surface contour</li>

<li>Can even detect a speaker’s heart rate (pulse in vessels in the tongue)</li>

</ul>

<p>Various psycholinguistic applications <span class="cite">McMillan &amp; Corley (2010); Palo (2019)</span></p>

<ul>

<li>Useful for detection of pre-speech articulation which may not be represented in the acoustic signal</li>

<li>Refinement of reaction times based on production</li>

</ul>

</div>

<div id="optical-flow-of" class="slide section level2">

<h1>Optical flow (OF)</h1>

<p>Related but more computationally complex: <strong>optical flow</strong>, which detects apparent motion between two frames <span class="cite">Horn &amp; Schunck (1981)</span></p>

<ul>

<li>Like PD, detects magnitude of motion; unlike PD, gives <em>direction</em> of motion as well</li>

<li>Produces a <strong>flow field</strong> for smaller elements in image, which can be averaged to a <strong>consensus</strong> <span class="cite">figure from Faytak, Moisik, &amp; Palo (2021)</span></li>

</ul>

<p><img src="./assets/media/satkit-of.png" width="450"></p>

</div>

<div id="applications-of-of" class="slide section level2">

<h1>Applications of OF</h1>

<p>Integrating the velocity signal gives amount of displacement for rigid bodies (i.e. the larynx) <span class="cite"> Moisik et al (2014); figure from Faytak, Moisik &amp; Palo (2021)</span></p>

<p><img src="./assets/media/satkit-movement.png" width="500"></p>

</div>



<div id="pixel-dimensionality-reduction" class="title-slide slide section level1">

<h1>Pixel dimensionality reduction</h1>



</div>

<div id="pixel-dimensionality-reduction-1" class="slide section level2">

<h1>Pixel dimensionality reduction</h1>

<p>Dimensionality reduction carried out not on ultrasound contours, but on entire <strong>ultrasound frames</strong></p>

<ul>

<li>Ultrasound frames are made up of tens of thousands of individual pixels</li>

<li>Each with a value indicating brightness (e.g. 0=black, 255=white)</li>

</ul>

<p>Brightness values can be used as raw data for dimensionality reduction</p>

</div>

<div id="pixels-and-scan-lines" class="slide section level2">

<h1>Pixels and scan lines</h1>

<p>Each ultrasound frame can be thought of as a matrix of width <span class="math inline"><em>w</em></span> by height <span class="math inline"><em>h</em></span></p>

<ul>

<li><span class="math inline"><em>w</em>, <em>h</em></span> values must be fixed throughout data collection (images must be same size)</li>

<li>Each column of pixels of shape <span class="math inline">1 × <em>h</em></span> represents a single <strong>scan line</strong> from the probe</li>

<li>Each scan line is reflectivity data from a single element in the probe</li>

</ul>

<p><img src="./assets/media/scanlines.jpg" width="375"></p>

</div>

<div id="high-dimensionality" class="slide section level2">

<h1>High dimensionality</h1>

<p>Each pixel at location <span class="math inline"><em>x</em>, <em>y</em></span> across data sets with the same frame size <span class="math inline"><em>w</em> × <em>h</em></span> can be thought of as a separate feature</p>

<ul>

<li>Meaning: each frame contains thousands, or tens of thousands, of features</li>

</ul>

<p>Challenge for <strong>feature selection</strong>:</p>

<ul>

<li>Bad idea to use every single pixel (curse of dimensionality)</li>

<li>Not easy to pick a small number of pixels which are good predictors of the phenomenon we’re studying</li>

<li>Solution: <strong>engineer new features</strong> which capture the variation</li>

</ul>

</div>

<div id="recap-dimensionality-reduction" class="slide section level2">

<h1>Recap: dimensionality reduction</h1>

<p>PCA outputs <strong>eigenvectors</strong> and <strong>eigenvalues</strong></p>

<ul>

<li>Eigenvectors (PCs) are patterns of variation uncovered in the data</li>

<li>Eigenvalues assign importance to explaining variation in the data</li>

<li>Observations can be transformed from naive basis into “PC space”</li>

</ul>

<p>We might recall this was applied to numerous acoustic measures in our first notebook</p>

<p>Eigenvector loadings:</p>

<p><img src="./assets/media/eigenvector-basic.png" width="550"></p>

<p>Data in PC space:</p>

<p><img src="./assets/media/notebook1-recap.png" width="600"></p>

</div>

<div id="calculating-image-eigenvectors" class="slide section level2">

<h1>Calculating image eigenvectors</h1>

<p>Method can be extended to <em>image data</em>: pixels in an image of shape <span class="math inline"><em>w</em> × <em>h</em></span> are treated as a <em>very long list</em> of features of length <span class="math inline"><em>w</em><em>h</em></span></p>

<ul>

<li>Basically, rows or columns are “unstacked” and “put end to end” to form a list</li>

</ul>

<p><img src="./assets/media/frametolist.png" width="600"></p>

<p>Like in the notebook’s dataframe:</p>

<ul>

<li>Each row in the dataframe is an observation (i.e. an ultrasound frame)</li>

<li>Each column in the dataframe is a feature (pixel in a given location)</li>

<li>The difference: <em>many more</em> columns/features</li>

</ul>

<p><img src="./assets/media/flatten.png" width="600"></p>

</div>

<div id="eigenvectors-as-eigenpictures" class="slide section level2">

<h1>Eigenvectors as eigenpictures</h1>

<p>Convert our length <span class="math inline"><em>w</em><em>h</em></span> eigenvectors back to <span class="math inline"><em>w</em> × <em>h</em></span> grids to get impact of associated PCs on pixel brightness <em>in physical space</em> <span class="cite">Sirovich &amp; Kirby (1987)</span></p>

<ul>

<li>Help us to evaluate what the discovered dimensions are</li>

<li>Famously applied to faces; eigenvectors converted back to original dimensions are called <strong>eigenfaces</strong> <span class="cite">Turk &amp; Pentland (1991)</span></li>

<li>Also applied to images of lips alone (eigenlips) <span class="cite">Bregler &amp; Konig (1994)</span></li>

</ul>

<p>We could reshape our notebook’s eigenvectors, but because feature number doesn’t correspond to physical space, it doesn’t really gain us anything</p>

<p><img src="./assets/media/reshape-basic.png" width="600"></p>

</div>

<div id="eigenfaces" class="slide section level2">

<h1>Eigenfaces</h1>

<p>When the features correspond to physical position (i.e. of pixels), eigenpictures tell us more: <span class="cite">figures from Turk &amp; Pentland (1991)</span></p>

<ul>

<li>We used red/blue diverging color scale in notebook; here white = positive, black = negative</li>

</ul>

<p><img src="./assets/media/turk-rotation.png" width="700"></p>

<p>Eigenfaces show variation in shape and size of facial features, hair, and setting (lighting, pose, etc)</p>

<ul>

<li>The most informative eigenfaces (PCs) mainly capture variation in hair and facial hair</li>

<li>Along with glasses, lighting, and probably skin tone</li>

</ul>

</div>

<div id="eigentongues" class="slide section level2">

<h1>Eigentongues</h1>

<p>Hueber et al (2007) coined <strong>eigentongues</strong>, from eigenfaces</p>

<ul>

<li>Take the PC loadings obtained from our “flattened” frame data, and “unflatten” them</li>

</ul>

<p><img src="./assets/media/eigentongue-basic2.png" width="700"></p>

<ul>

<li>Covariation of pixel brightness with PCs shows variation in <em>tongue contour</em> position

<ul>

<li>And any other patterns in data set (hyoid shadow position, internal musculature of tongue, …)</li>

</ul></li>

<li>Here, higher PC1 “moves” upper part of contour to the right; lower PC1 “moves” it to the left</li>

</ul>

</div>

<div id="eigentongues-1" class="slide section level2">

<h1>eigentongues</h1>

<p>Another example, taken from <em>all</em> tongue motions in a corpus, at a lower spatial resolution <span class="cite">figures from Hueber et al (2007)</span></p>

<ul>

<li>Again, white = positive covariation with PC score; black = negative</li>

<li>Higher PC1 score makes brighter pixels light up</li>

<li>Lower PC1 score makes darker pixels light up</li>

</ul>

<p><img src="./assets/media/hueber-all.png" width="700"></p>

</div>

<div id="pc-scores" class="slide section level2">

<h1>PC scores</h1>

<p>We can then characterize the ultrasound image data set in PC-space terms (PC scores)</p>

<p>show ex</p>

<p>projection math here?</p>

<ul>

<li>Hoole &amp; Pouplier as an example of direct analysis of PC1</li>

<li>regression analysis</li>

<li>correlation analysis</li>

</ul>

</div>

<div id="time-series-of-pc-scores" class="slide section level2">

<h1>Time series of PC scores</h1>

<p>If the data include all frames in a target interval, then the PCs can be used to track dynamical changes across the duration of the target interval</p>

<ul>

<li>Mielke &amp; Carignan</li>

<li>Kochetov, Faytak, Nara</li>

</ul>

</div>

<div id="reconstruction-using-eigenvectors" class="slide section level2">

<h1>Reconstruction using eigenvectors</h1>

<p>Reconstruction math here?</p>

<p>insert Berry reconst. equation here (doesn’t typeset properly)</p>

<p>Reconstruction of basis data from linear combination and weighting of eigentongues is easily achieved</p>

<p>Reconstruction in Faytak et al. 2020</p>

<p><img src="./assets/media/turk-figs1-2.png" width="700"></p>

</div>

<div id="reconstruction-and-missing-data" class="slide section level2">

<h1>Reconstruction and missing data</h1>

<p>Works particularly nicely when part of an observation’s data is missing, but similar complete data was used to generate the eigenvectors</p>

<p>Masked test data and its reconstruction: <span class="cite">figures from Turk &amp; Pentland (1991)</span></p>

<p><img src="./assets/media/missing-recon.png" width="400"></p>

<p>Ground truth (which was used in training set for eigenvectors):</p>

<p><img src="./assets/media/missing-groundtruth.png" width="200"></p>

</div>

<div id="ultrasound-use" class="slide section level2">

<h1>Ultrasound use</h1>

<p>Reconstruction of ultrasound images creates a <em>denoised</em> reconstruction of the observation</p>

<ul>

<li>Focus on pixels which vary most interestingly</li>

</ul>

<p>image</p>

<p>Can also be used to get <em>average</em> denoised reconstruction for a <em>set</em> of frames</p>

<p>image</p>

</div>

<div id="caveats" class="slide section level2">

<h1>Caveats</h1>

<p>Eigenimage analysis can work with new data only if images are (more or less) contained within training set</p>

<ul>

<li>Turk &amp; Pentland eigenface data set is mostly white, entirely men</li>

<li>Resist the urge to uncritically use a training set on new data</li>

<li>Treating training data or algorithms (like PCA) as fair and neutral can have unintended negative consequences <span class="cite">Buolamwini &amp; Gebru (2018)</span></li>

</ul>

</div>



<div id="wrapping-up" class="title-slide slide section level1">

<h1>Wrapping up</h1>



</div>

<div id="a-summary-slide" class="slide section level2">

<h1>A summary slide</h1>

<p>Analysis of entire ultrasound images</p>

<ul>

<li>Using dimensionality reduction (PCA) over all pixels</li>

<li>Measuring change in pixel intensity</li>

<li>Measuring optical flow</li>

</ul>

</div>

<div id="pros" class="slide section level2">

<h1>Pros</h1>

<p>Very efficient once the basics are mastered</p>

<ul>

<li>Speedy (big advantage over basic contour extraction)</li>

<li>Very replicable</li>

</ul>

<p>Potentially more informative in some respects than contours</p>

<ul>

<li>Especially for data where parts of tongue contour aren’t visible</li>

</ul>

</div>

<div id="convergence-with-other-methods" class="slide section level2">

<h1>Convergence with other methods</h1>

<p>converging on common analysis across methods: pixel and pixel dimred methods easy to use on other data types</p>

<ul>

<li>MRI <span class="cite">Oh &amp; Lee (2018)</span></li>

<li>Video of face, especially lips <span class="cite">Krause et al (2020)</span></li>

</ul>

</div>

<div id="cons" class="slide section level2">

<h1>Cons</h1>

<p>Best suited to analyses of relative similarity and difference of sounds</p>

<ul>

<li>Somewhat limited</li>

<li>Fairly different from some approaches to analysis (“engineering-y”)</li>

</ul>

<p>If using a train-test approach, hinges on quality of training data</p>

<ul>

<li>While there are only so many poses the tongue can take,</li>

</ul>

</div>

<div id="references" class="slide section level2">

<h1>References</h1>

<p>Bregler, C. &amp; Konig, Y. (1994). “Eigenlips” for robust speech recognition. In <em>Proceedings of ICASSP ’94 Vol. 2</em>. <a href="https://doi.org/10.1109/ICASSP.1994.389567">DOI</a></p>

<p>Buolamwini, J. &amp; Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. <em>Proceedings of the 1st Conference on Fairness, Accountability and Transparency</em>, in <em>Proceedings of Machine Learning Research</em> 81, 77-91. <a href="https://proceedings.mlr.press/v81/buolamwini18a.html">PDFs</a></p>

<p>Danilouchkine, M., Mastik, F. &amp; van der Steen, A. (2009). A study of coronary artery rotational motion with dense scale-space optical flow in intravascular ultrasound. <em>Physics in Medicine and Biology</em>, 54(6), 1397–1418. <a href="https://doi.org/10.1088/0031-9155/54/6/002">DOI</a></p>

<p>Davidson, L. (2006). Comparing tongue shapes from ultrasound imaging using smoothing spline analysis of variance. <em>The Journal of the Acoustical Society of America</em>, 120, pp. 407–415. <a href="https://doi.org/10.1121/1.2205133">DOI</a></p>

<p>Eshky, A., Cleland, J., Ribeiro, M., Sugden, E., Richmond, K. &amp; Renals, S. (2021). Automatic audiovisual synchronisation for ultrasound tongue imaging. <em>Speech Communication</em>, 132, 83-95. <a href="https://doi.org/10.1016/j.specom.2021.05.008">DOI</a></p>

<p>Faytak, M., Moisik, S. &amp; Palo, P. (2021). The Speech Articulation Toolkit (SATKit): Ultrasound image analysis in Python. In <em>Proceedings of ISSP 12</em>, 234-237. <a href="https://issp2020.yale.edu/ProcISSP2020.pdf">PDF</a></p>

<p>Heyne, M., Derrick, D., &amp; Al-Tamimi, J. (2019). Native language influence on brass instrument performance: An application of generalized additive mixed models (GAMMs) to midsagittal ultrasound images of the tongue. <em>Frontiers in Psychology</em>, 2597. <a href="https://doi.org/10.3389/fpsyg.2019.02597">DOI</a></p>

<p>Horn, B., &amp; Schunck, B. (1981). Determining optical flow. <em>Artificial Intelligence</em>, 17(1), 185–203. <a href="https://doi.org/10.1016/0004-3702(81)90024-2">DOI</a></p>

<p>Hueber, T., Aversano, G., Chollet, G., Denby, B., Dreyfus, G., Oussar, Y., Roussel, P. &amp; Stone, M. (2007). Eigentongue feature extraction for an ultrasound-based silent speech interface. In <em>Proceedings of ICASSP ’07 Vol. 1</em>. <a href="https://doi.org/10.1109/ICASSP.2007.366140">DOI</a></p>

<p>Iskarous, K. (2005). Detecting the edge of the tongue: A tutorial. <em>Clinical Linguistics &amp; Phonetics</em>, 19(6-7), 555-565. <a href="https://doi.org/10.1080/02699200500113871">DOI</a></p>

<p>Krause, P., Kay, C. &amp; Kawamoto, A., (2020) Automatic motion tracking of lips using digital video and OpenFace 2.0, <em>Laboratory Phonology</em> 11(1), 9. <a href="https://doi.org/10.5334/labphon.232">DOI</a></p>

<p>McMillan, C. &amp; Corley, M. (2010). Cascading influences on the production of speech: Evidence from articulation. <em>Cognition</em>, 117(3), 243–260. <a href="https://doi.org/10.1016/j.cognition.2010.08.019">DOI</a></p>

<p>Moisik, S., Lin, H., &amp; Esling, J. (2014). A study of laryngeal gestures in Mandarin citation tones using simultaneous laryngoscopy and laryngeal ultrasound (SLLUS). <em>JIPA</em>, 44(1), 21– 58. <a href="https://doi.org/10.1017/S0025100313000327">DOI</a></p>

<p>Oh, M., &amp; Lee, Y. (2018). ACT: An Automatic Centroid Tracking tool for analyzing vocal tract actions in real-time magnetic resonance imaging speech production data. <em>The Journal of the Acoustical Society of America</em>, 144(4), EL290-EL296. <a href="https://doi.org/10.1121/1.5057367">DOI</a></p>

<p>Palo, P. (2019). Measuring pre-speech articulation. Doctoral dissertation, Queen Margaret University. <a href="https://eresearch.qmu.ac.uk/handle/20.500.12289/10163">PDF</a></p>

<p>Stone, M. (2005). A guide to analysing tongue motion from ultrasound images. <em>Clinical Linguistics &amp; Phonetics</em>, 19(6-7), 455-501. <a href="https://doi.org/10.1080/02699200500113558">DOI</a></p>

<p>Turk, M. &amp; Pentland, A. (1991). Eigenfaces for recognition. <em>Journal of Cognitive Neuroscience</em>, 3(1), 71-86. <a href="https://doi.org/10.1162/jocn.1991.3.1.71">DOI</a></p>

<p>Weller, J., Faytak, M., Steffman, J., Mayer, C., Teixeira, G. &amp; Tankou, R. (to appear). Supralaryngeal articulation across voicing and aspiration in Yemba vowels. In <em>Proceedings of ACAL 51/52</em>.</p>

<p>Wrench, A., &amp; Scobbie, J. (2008). High-speed cineloop ultrasound vs. video ultrasound tongue imaging: Comparison of front and back lingual gesture location and relative timing. In <em>Proceedings of ISSP 8</em>. <a href="https://eresearch.qmu.ac.uk/handle/20.500.12289/2012">PDF</a></p>

</div>

</body>

</html>
